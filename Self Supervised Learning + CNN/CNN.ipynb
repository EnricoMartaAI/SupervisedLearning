{"metadata":{"colab":{"provenance":[],"collapsed_sections":["ur2oVtALpDBA","NUWAnJvzpDBQ","z_4pXEy_7go7","s79NHZSfhTx6","5BBja0Ulm5gs","VUweYS3opDBi"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8571202,"sourceType":"datasetVersion","datasetId":5125058}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# LIBRARIES"],"metadata":{"id":"ur2oVtALpDBA"}},{"cell_type":"code","source":["#!pip install torchsummary"],"metadata":{"jupyter":{"source_hidden":true},"id":"6ppY-Cg_pDBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import json\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import pandas as pd\n","import scipy\n","import torch\n","import collections\n","from PIL import Image\n","import torch.nn as nn\n","from torchvision.models.mobilenetv2 import Conv2dNormActivation, InvertedResidual\n","from torchvision.models import mobilenet_v2\n","from torchvision.models import MobileNet_V2_Weights\n","import pickle\n","from torchsummary import summary\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import itertools\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import precision_score, recall_score, f1_score"],"metadata":{"jupyter":{"source_hidden":true},"id":"WhevpAxcpDBN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CUDA"],"metadata":{"id":"NUWAnJvzpDBQ"}},{"cell_type":"code","source":["# Train on GPU if available\n","train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n","\n","device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n","\n","print(device)"],"metadata":{"jupyter":{"source_hidden":true},"id":"4hy0dlzOpDBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# IMPORT THE DATASET\n"],"metadata":{"id":"z_4pXEy_7go7"}},{"cell_type":"code","source":["'''\n","\n","This part of the code serves the purpose of dowloading\n","the dataset in a proper way for kaggle. A file json is created\n","to download the dataset from kaggle.\n","\n","'''\n","\n","# Create file kaggle.json to download from Kaggle\n","kaggle_json = {\n","    \"username\": \"enricomarta\",\n","    \"key\": \"84d386ad7a7c02ed584cae00085f289b\"\n","}\n","\n","# Path of kaggle.json\n","kaggle_json_path = '/kaggle/working/kaggle.json'\n","\n","# Write on kaggle.json\n","with open(kaggle_json_path, 'w') as file:\n","    json.dump(kaggle_json, file)\n","\n","# Kaggle's enviornment\n","os.environ['KAGGLE_CONFIG_DIR'] = '/kaggle/working'\n","\n","# Correctness of the information\n","!chmod 600 /kaggle/working/kaggle.json\n","\n","print(\"Downloading the dataset from Kaggle...\")\n","!kaggle competitions download -c ifood-2019-fgvc6 -p /kaggle/working --force"],"metadata":{"id":"zaez6zdn_e55","execution":{"iopub.status.busy":"2024-06-14T10:08:25.820118Z","iopub.execute_input":"2024-06-14T10:08:25.820540Z","iopub.status.idle":"2024-06-14T10:08:40.815247Z","shell.execute_reply.started":"2024-06-14T10:08:25.820515Z","shell.execute_reply":"2024-06-14T10:08:40.814315Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unzip files\n","!unzip -o ifood-2019-fgvc6.zip -d /kaggle/working > /dev/null\n","!unzip -o train_set.zip -d /kaggle/working > /dev/null\n","\n","!ls"],"metadata":{"id":"CYTu3nKmJFXO","execution":{"iopub.status.busy":"2024-06-14T10:15:46.079492Z","iopub.execute_input":"2024-06-14T10:15:46.079823Z","iopub.status.idle":"2024-06-14T10:15:48.924041Z","shell.execute_reply.started":"2024-06-14T10:15:46.079798Z","shell.execute_reply":"2024-06-14T10:15:48.923118Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove unnecesary files for the project\n","!rm class_list.txt ifood-2019-fgvc6.zip sample_submission.csv ifood2019_sample_submission.csv test_set.zip train_set.zip val_labels.csv val_set.zip"],"metadata":{"id":"woBqq5JJiQpd","execution":{"iopub.status.busy":"2024-06-14T10:10:03.799521Z","iopub.execute_input":"2024-06-14T10:10:03.799901Z","iopub.status.idle":"2024-06-14T10:10:05.584874Z","shell.execute_reply.started":"2024-06-14T10:10:03.799864Z","shell.execute_reply":"2024-06-14T10:10:05.583649Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PRE-PROCESSING"],"metadata":{"id":"s79NHZSfhTx6"}},{"cell_type":"code","source":["'''\n","Here training and validation set are created and saved.\n","Then number of images contained in both are providded.\n","'''\n","\n","# path of the csv file of the labels\n","file_path = '/kaggle/working/train_labels.csv'\n","# upload the csv file\n","data = pd.read_csv(file_path)\n","# Split the data into training and validation sets (30% for validation)\n","train_data, validation_data = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=1)\n","\n","# Save the validation set\n","validation_data.to_csv('validation_set.csv', index=False)\n","\n","# Save the training set\n","train_data.to_csv('training_set.csv', index=False)\n","\n","num_train_rows = train_data.shape[0]\n","num_validation_rows = validation_data.shape[0]\n","print(f\"Number of rows in the training set: {num_train_rows}\")\n","print(f\"Number of rows in the validation set: {num_validation_rows}\")"],"metadata":{"execution":{"iopub.status.busy":"2024-06-14T10:16:01.935042Z","iopub.execute_input":"2024-06-14T10:16:01.935413Z","iopub.status.idle":"2024-06-14T10:16:02.224641Z","shell.execute_reply.started":"2024-06-14T10:16:01.935383Z","shell.execute_reply":"2024-06-14T10:16:02.223616Z"},"jupyter":{"source_hidden":true},"trusted":true,"id":"CHSQN-7npDBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Here the 'final dataset' is created, augmenting\n","under-represented classes via duplication, and then saved.\n","'''\n","\n","# Path of the CSV file of the training set\n","file_path = '/kaggle/working/training_set.csv'\n","\n","# Upload the CSV file\n","data = pd.read_csv(file_path)\n","\n","# Number of rows\n","num_rows = data.shape[0]\n","print(f\"Number of rows: {num_rows}\")\n","\n","# Number of labels\n","num_labels = data['label'].nunique()\n","print(f\"Number of labels: {num_labels}\")\n","\n","# Number of observations for each label\n","images_per_label = data['label'].value_counts()\n","print(\"Number of images per label (ascending order):\")\n","print(images_per_label.sort_values())\n","\n","# Find classes with fewer than 300 observations\n","classes_to_duplicate = images_per_label[images_per_label < 300].index\n","\n","# DataFrame for the duplicate images\n","duplicated_data = []\n","\n","# For each label that doesn't have enough images\n","for label in classes_to_duplicate:\n","    rows_to_duplicate = data[data['label'] == label]\n","    # Calculate how many more images are needed to reach 300\n","    count_needed = 300 - len(rows_to_duplicate)\n","    # Generates a sample of duplicate rows from the current class\n","    duplicated_rows = rows_to_duplicate.sample(n=count_needed, replace=True, random_state=1)\n","    # Concatenates the original rows together with the duplicated rows\n","    duplicated_data.append(pd.concat([rows_to_duplicate, duplicated_rows], ignore_index=True))\n","\n","# Merge all positions contained in the list\n","final_duplicated_data = pd.concat(duplicated_data, ignore_index=True)\n","\n","# Well represented data (already have 300 or more observations)\n","well_represented_data = data[data['label'].value_counts()[data['label']].values >= 300]\n","\n","# Merge to obtain the final new dataframe\n","final_data = pd.concat([well_represented_data, final_duplicated_data], ignore_index=True)\n","\n","# Create the new file CSV\n","final_file_path = '/kaggle/working/modified_train_labels.csv'\n","final_data.to_csv(final_file_path, index=False)\n","\n","\n","data_modified = pd.read_csv(final_file_path)\n","\n","# New number of images\n","num_rows_modified = len(data_modified)\n","print(f\"Number of images in modified file: {num_rows_modified}\")\n","# Number of observations for each label\n","images_per_label_mod = data_modified['label'].value_counts()\n","print(\"Number of images per label (ascending order):\")\n","print(images_per_label_mod.sort_values())"],"metadata":{"id":"x_tEZnKK9baF","execution":{"iopub.status.busy":"2024-06-14T10:16:06.802207Z","iopub.execute_input":"2024-06-14T10:16:06.802566Z","iopub.status.idle":"2024-06-14T10:16:07.155789Z","shell.execute_reply.started":"2024-06-14T10:16:06.802537Z","shell.execute_reply":"2024-06-14T10:16:07.154778Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Here the class 'FoodDataset is created.\n","It provides an appropriate way to manage images.\n","'''\n","\n","\n","class FoodDataset(Dataset):\n","\n","    def __init__(self, root_dir, transform, split):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory containing the images.\n","            transform (callable, optional): Transformation to be applied to the images.\n","            split (string): Split type (\"train\" or \"val\").\n","        \"\"\"\n","        self.split = split\n","        self.root_dir = root_dir\n","        self.data = self.get_data_train()\n","        self.data_v = self.get_data_val()\n","\n","        if split == \"train\":\n","            self.data = self.data\n","        elif split == \"val\":\n","            self.data = self.data_v\n","        self.transform = transform\n","\n","    def get_data_train(self):\n","        df = pd.read_csv('/kaggle/working/training_set.csv')\n","        # Construct full image paths using vectorized operations\n","        df['img_name'] = self.root_dir + \"/\" + df['img_name']\n","        # Filter for images with existing paths using vectorized boolean indexing\n","        df = df[df['img_name'].apply(os.path.exists)]\n","        return df.to_numpy()  # Return preprocessed data as a NumPy array\n","\n","    def get_data_val(self):\n","        df = pd.read_csv('/kaggle/working/validation_set.csv')\n","        # Construct full image paths using vectorized operations\n","        df['img_name'] = self.root_dir + \"/\" + df['img_name']\n","        # Filter for images with existing paths using vectorized boolean indexing\n","        df = df[df['img_name'].apply(os.path.exists)]\n","        return df.to_numpy()  # Return preprocessed data as a NumPy array\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset (number of samples).\n","\n","        This method overrides the default behavior of `len` for the dataset object.\n","        It simply returns the length of the internal `data` list, which represents\n","        the preprocessed data after loading and filtering.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a sample at a given index.\n","\n","        This method overrides the default behavior of indexing for the dataset object.\n","        It takes an index `idx` and performs the following:\n","            1. Accesses the image name and num_class at the specified index from `self.data`.\n","            2. Opens the image using `Image.open` with the full path constructed by\n","               combining `self.root_dir` and `img_name`.\n","            3. Applies the defined transformation (`self.transform`) to the image.\n","            4. Creates a dictionary `sample` containing the preprocessed image (`image`)\n","               and the num_class as a PyTorch tensor (`torch.tensor(num_class).float()`).\n","            5. Returns the constructed `sample` dictionary.\n","        \"\"\"\n","        img_name, num_class = self.data[idx]\n","        image = Image.open(os.path.join(self.root_dir, img_name))\n","        image = self.transform(image)\n","\n","        sample = {'img_name': image, 'labels': torch.tensor(num_class).float()}\n","        return sample\n"],"metadata":{"id":"zAFIbNQyCWYq","execution":{"iopub.status.busy":"2024-06-14T10:17:02.868855Z","iopub.execute_input":"2024-06-14T10:17:02.869324Z","iopub.status.idle":"2024-06-14T10:17:02.882246Z","shell.execute_reply.started":"2024-06-14T10:17:02.869292Z","shell.execute_reply":"2024-06-14T10:17:02.881295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Functions for data transformation are provided,\n","distinguishing between trainin and validation sets.\n","Creation of Dataloaders.\n","'''\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize images to 256x256\n","    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with a probability of 50%\n","    transforms.RandomAffine(degrees=0, shear=0.2),  # Apply random shear transformations\n","    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0), ratio=(1.0, 1.0)),  # Apply random zoom transformations\n","    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n","    transforms.Normalize(  # Normalize pixel values based on ImageNet statistics\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","transform_val = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize images to 256x256 (consistent with training)\n","    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n","    transforms.Normalize(  # Normalize pixel values using the same statistics\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","\n","# Set batch size\n","bs = 100\n","\n","# Create datasets for training, validation\n","trainset = FoodDataset(\"/kaggle/working/train_set\", transform_train, split=\"train\")\n","valset = FoodDataset(\"/kaggle/working/train_set\", transform_val, split=\"val\")\n","\n","# Create data loaders for efficient batch training and evaluation\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n","\n","# Print dataset and dataloader lengths (number of samples and batches)\n","print(f\"Number of training samples: {len(trainloader) * bs}\")\n","print(f\"Number of validation samples: {len(valloader)}\")"],"metadata":{"id":"cWihFii-IvC7","execution":{"iopub.status.busy":"2024-06-14T10:17:07.947108Z","iopub.execute_input":"2024-06-14T10:17:07.947467Z","iopub.status.idle":"2024-06-14T10:17:09.161798Z","shell.execute_reply.started":"2024-06-14T10:17:07.947432Z","shell.execute_reply":"2024-06-14T10:17:09.160738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CNN"],"metadata":{"id":"5BBja0Ulm5gs"}},{"cell_type":"markdown","source":["https://paperswithcode.com/lib/torchvision/mobilenet-v2\n","https://pytorch.org/vision/main/_modules/torchvision/models/mobilenetv2.html"],"metadata":{"id":"r0JSmgQ2pDBe"}},{"cell_type":"code","source":["'''\n","Here the architecture of the CNN is provided.\n","Transfer learning option available.\n","'''\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Let's create a reduced version of MobileNetV2\n","        self.features = nn.Sequential(\n","            Conv2dNormActivation(3, 32, kernel_size=3, stride=2, padding=1),\n","            InvertedResidual(32, 16, stride=1, expand_ratio=1),\n","            InvertedResidual(16, 24, stride=2, expand_ratio=6),\n","            InvertedResidual(24, 24, stride=1, expand_ratio=6),\n","            InvertedResidual(24, 32, stride=2, expand_ratio=6),\n","            InvertedResidual(32, 32, stride=1, expand_ratio=6),\n","            InvertedResidual(32, 32, stride=1, expand_ratio=6),\n","            InvertedResidual(32, 64, stride=2, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 96, stride=1, expand_ratio=6),\n","            InvertedResidual(96, 96, stride=2, expand_ratio=6),\n","            InvertedResidual(96, 96, stride=2, expand_ratio=6),\n","            Conv2dNormActivation(96, 800, kernel_size=1, stride=1, padding=0)\n","        )\n","\n","        # Define classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(800, 445),\n","            nn.GELU(),\n","            nn.Linear(445, 32),\n","            nn.GELU(),\n","            nn.Linear(32, 251)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        # adaptive_avg_pool2d allows to reduce the dimension of x to (batch_size, channels, 1, 1)\n","        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).flatten(1)\n","        # After flatten the dimensions of x is (batch_size, channels)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","# If we want to start with a knowledge base (transfer learning)\n","def initialize_pretrained_weights(model, pretrained_model):\n","    # Get the layers of our model\n","    model_layers = list(model.features.children())\n","    # Get the layers of the MobileNetV2\n","    pretrained_layers = list(pretrained_model.features.children())\n","    # Iterate over the layers of both models at the same time\n","    for model_layer, pretrained_layer in zip(model_layers, pretrained_layers):\n","        # Check if the layers are of the same type\n","        if isinstance(model_layer, type(pretrained_layer)):\n","            # Copy the weights from the pretrained layer to the new model layer\n","            model_layer.load_state_dict(pretrained_layer.state_dict())\n","\n","\n","# Load MobileNetV2, uncomment if needed\n","#pretrained_mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n","# Load the new model\n","net = Net()\n","# Copy the weights: uncomment if needed\n","#initialize_pretrained_weights(net, pretrained_mobilenet)\n","\n","net.to(device)\n","_ = net"],"metadata":{"id":"RNi9bZOqVQFg","execution":{"iopub.status.busy":"2024-06-14T10:17:14.678315Z","iopub.execute_input":"2024-06-14T10:17:14.678677Z","iopub.status.idle":"2024-06-14T10:17:14.846398Z","shell.execute_reply.started":"2024-06-14T10:17:14.678647Z","shell.execute_reply":"2024-06-14T10:17:14.845552Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uncomment if you want to print model summary\n","#summary(net, (3, 256, 256))  # Input shape (channels, height, width)"],"metadata":{"id":"JUKJ13cgf7Ju","execution":{"iopub.status.busy":"2024-06-14T10:14:18.364342Z","iopub.execute_input":"2024-06-14T10:14:18.364785Z","iopub.status.idle":"2024-06-14T10:14:18.397034Z","shell.execute_reply.started":"2024-06-14T10:14:18.364752Z","shell.execute_reply":"2024-06-14T10:14:18.396220Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRAINING and EVALUATION"],"metadata":{"id":"VUweYS3opDBi"}},{"cell_type":"code","source":["'''\n","Define training parameters (epochs, loss function, optimizer, and scheduler)\n","'''\n","\n","epochs = 60  # Number of training epochs\n","criterion = nn.CrossEntropyLoss()  # Cross Entropy as Loss function for classification\n","optimizer = optim.Adam(net.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n","scheduler = CosineAnnealingLR(optimizer,\n","                              T_max=len(trainloader) * epochs,  # Maximum number of iterations for scheduler\n","                              eta_min=1e-5)  # Minimum learning rate for scheduler"],"metadata":{"id":"1jPH9gqynToX","execution":{"iopub.status.busy":"2024-06-14T10:17:36.838397Z","iopub.execute_input":"2024-06-14T10:17:36.838744Z","iopub.status.idle":"2024-06-14T10:17:36.847442Z","shell.execute_reply.started":"2024-06-14T10:17:36.838718Z","shell.execute_reply":"2024-06-14T10:17:36.846488Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Training and Validation section.\n","'''\n","\n","# Uncomment if you need to complete the training with pre-saved model\n","#net.load_state_dict(torch.load(\"/kaggle/input/vecchio/net_best.pth\"))\n","\n","# Parameters for the early stopping procedure\n","patience = 3\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","# List to evaluate the final model\n","all_preds = []\n","all_labels = []\n","\n","# Training\n","for epoch in range(epochs):\n","    running_loss = []  # List to store training loss for each batch\n","    net.train()  # Set the model to training mode\n","    for i, data in enumerate(trainloader):\n","        # Get inputs and labels from the data loader\n","        inputs, labels = data[\"img_name\"], data[\"labels\"]\n","        inputs = inputs.to(device)\n","        labels = labels.to(device).long()\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = net(inputs)\n","        # Calculate loss\n","        loss = criterion(outputs.squeeze(), labels)\n","        # Backward pass and parameter update\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        # Print statistics (every 10% of the training data)\n","        running_loss.append(loss.item())\n","        if (i + 1) % (len(trainloader) // 10) == 0:\n","            print('%d, [%d, %d] loss: %.4f\\tlr: %.6f' %\n","                  (epoch + 1, i + 1, len(trainloader), np.mean(running_loss), optimizer.param_groups[-1]['lr']))\n","            running_loss = []\n","\n","    # Validation\n","    running_loss = []  # List to store validation loss for each batch\n","    net.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","    for i, data in enumerate(valloader):\n","        # Get inputs and labels from the data loader\n","        inputs, labels = data[\"img_name\"], data[\"labels\"]\n","        inputs = inputs.to(device)\n","        labels = labels.to(device).long()\n","\n","        # Forward pass with gradient suppression\n","        with torch.no_grad():\n","            outputs = net(inputs)  # Get model predictions without calculating gradients\n","            # Finds the class with the highest probability for each image in the batch.\n","            _, predicted = torch.max(outputs.cpu(), 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            # Update total number of test images\n","            total += labels.size(0)  # label.size(0) gives the batch size\n","            # Count correct predictions\n","            correct += (predicted == labels.cpu()).sum().item()  # Count true positives\n","\n","        loss = criterion(outputs, labels)  # Calculate Cross Entropy loss\n","        running_loss.append(loss.item())\n","\n","    val_loss = np.mean(running_loss)\n","    print('Validation loss: %.6f' % val_loss)\n","    # Calculate and print accuracy\n","    accuracy = 100 * correct / total\n","    print(f'Validation accuracy: {accuracy:.2f} %')\n","\n","    # Early stopping if no improvement in validation loss\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        torch.save(net.state_dict(), \"net_best.pth\")\n","        print(f\"Epoch {epoch+1}: Validation loss improved to {val_loss:.6f}. Model saved.\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\"Epoch {epoch+1}: No improvement in validation loss ({val_loss:.6f}). Epochs without improvement: {epochs_no_improve}/{patience}\")\n","\n","    if epochs_no_improve == patience:\n","        print(f\"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.6f}\")\n","        break\n","\n","print('Finished Training')"],"metadata":{"id":"_gR6jpvkna9C","execution":{"iopub.status.busy":"2024-06-14T10:18:39.829431Z","iopub.execute_input":"2024-06-14T10:18:39.829816Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}