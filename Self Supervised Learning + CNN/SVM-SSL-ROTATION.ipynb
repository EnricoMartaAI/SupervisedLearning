{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["-plpy6-vnbpP","xtIJF4CEneni","ePTneRCKnjup","DQx6q7jbnmzh","bE3Yg6HZoD4v","61AjEHXUoTJC","emnndv9VownF","krF-7QYipAz3"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# LIBRARIES"],"metadata":{"id":"-plpy6-vnbpP"}},{"cell_type":"code","source":["import torch\n","import json\n","import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import scipy\n","import collections\n","from PIL import Image\n","import pickle\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models.mobilenetv2 import Conv2dNormActivation, InvertedResidual\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import itertools\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import precision_score, recall_score, f1_score\n"],"metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:11:19.723872Z","iopub.execute_input":"2024-06-16T21:11:19.724222Z","iopub.status.idle":"2024-06-16T21:11:25.243229Z","shell.execute_reply.started":"2024-06-16T21:11:19.724196Z","shell.execute_reply":"2024-06-16T21:11:25.242193Z"},"trusted":true,"id":"kO2_iKXcnSHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CUDA"],"metadata":{"id":"xtIJF4CEneni"}},{"cell_type":"code","source":["# Train on GPU if available\n","train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n","\n","device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n","\n","print(device)"],"metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:11:25.244998Z","iopub.execute_input":"2024-06-16T21:11:25.245402Z","iopub.status.idle":"2024-06-16T21:11:25.300284Z","shell.execute_reply.started":"2024-06-16T21:11:25.245375Z","shell.execute_reply":"2024-06-16T21:11:25.299238Z"},"trusted":true,"id":"-q04HfIjnSHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# IMPORT THE DATASET"],"metadata":{"id":"ePTneRCKnjup"}},{"cell_type":"code","source":["'''\n","\n","This part of the code serves the purpose of dowloading\n","the dataset in a proper way for kaggle. A file json is created\n","to download the dataset from kaggle.\n","\n","'''\n","\n","# Create file kaggle.json to download from Kaggle\n","kaggle_json = {\n","    \"username\": \"enricomarta\",\n","    \"key\": \"84d386ad7a7c02ed584cae00085f289b\"\n","}\n","\n","# Path of the kaggle.json\n","kaggle_json_path = '/kaggle/working/kaggle.json'\n","\n","# Write on kaggle.json\n","with open(kaggle_json_path, 'w') as file:\n","    json.dump(kaggle_json, file)\n","\n","# Kaggle's enviornment\n","os.environ['KAGGLE_CONFIG_DIR'] = '/kaggle/working'\n","\n","# Correctness of the information\n","!chmod 600 /kaggle/working/kaggle.json\n","\n","print(\"Downloading the dataset from Kaggle...\")\n","!kaggle competitions download -c ifood-2019-fgvc6 -p /kaggle/working --force"],"metadata":{"execution":{"iopub.status.busy":"2024-06-16T21:11:25.301496Z","iopub.execute_input":"2024-06-16T21:11:25.301822Z"},"trusted":true,"id":"ut2rdk8cnSHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# unzip files\n","!unzip -o ifood-2019-fgvc6.zip -d /kaggle/working > /dev/null\n","!unzip -o train_set.zip -d /kaggle/working > /dev/null\n","\n","!ls"],"metadata":{"trusted":true,"id":"XnOByH-InSHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove unnecesary files for the project\n","!rm class_list.txt ifood-2019-fgvc6.zip sample_submission.csv ifood2019_sample_submission.csv test_set.zip train_set.zip val_labels.csv val_set.zip"],"metadata":{"trusted":true,"id":"m5ymbNjYnSHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PREPROCESSING"],"metadata":{"id":"DQx6q7jbnmzh"}},{"cell_type":"code","source":["'''\n","Here training and validation set are created and saved.\n","Then number of images contained in both are providded.\n","'''\n","\n","# path of the csv file\n","file_path = '/kaggle/working/train_labels.csv'\n","data = pd.read_csv(file_path)\n","\n","# Split the data into training and validation sets (30% for validation)\n","train_data, validation_data = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42)\n","\n","# Save the validation set\n","validation_data.to_csv('validation_set.csv', index=False)\n","\n","# Save the training set\n","train_data.to_csv('training_set.csv', index=False)\n","\n","num_train_rows = train_data.shape[0]\n","num_validation_rows = validation_data.shape[0]\n","print(f\"Number of rows in the training set: {num_train_rows}\")\n","print(f\"Number of rows in the validation set: {num_validation_rows}\")"],"metadata":{"trusted":true,"id":"lX2qnyRdnSHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Here the 'final dataset' is created, augmenting\n","under-represented classes via duplication, and then saved.\n","'''\n","# Path of the CSV file\n","file_path = '/kaggle/working/training_set.csv'\n","\n","# Upload the CSV file\n","data = pd.read_csv(file_path)\n","\n","# Number of rows\n","num_rows = data.shape[0]\n","print(f\"Number of rows: {num_rows}\")\n","\n","# Number of labels\n","num_labels = data['label'].nunique()\n","print(f\"Number of labels: {num_labels}\")\n","\n","# Number of observations for each label\n","images_per_label = data['label'].value_counts()\n","print(\"Number of images per label (ascending order):\")\n","print(images_per_label.sort_values())\n","\n","# Find classes with fewer than 300 observations\n","classes_to_duplicate = images_per_label[images_per_label < 300].index\n","\n","# DataFrame for the duplicate images\n","duplicated_data = []\n","\n","# For each label that doesn't have enough images\n","for label in classes_to_duplicate:\n","    rows_to_duplicate = data[data['label'] == label]\n","    # Calculate how many more images are needed to reach 300\n","    count_needed = 300 - len(rows_to_duplicate)\n","    # Generates a sample of duplicate rows from the current class\n","    duplicated_rows = rows_to_duplicate.sample(n=count_needed, replace=True, random_state=1)\n","    # Concatenates the original rows together with the duplicated rows\n","    duplicated_data.append(pd.concat([rows_to_duplicate, duplicated_rows], ignore_index=True))\n","\n","# Merge all positions contained in the list\n","final_duplicated_data = pd.concat(duplicated_data, ignore_index=True)\n","\n","# Well represented data (already have 300 or more observations)\n","well_represented_data = data[data['label'].value_counts()[data['label']].values >= 300]\n","\n","# Merge to obtain the final new dataframe\n","final_data = pd.concat([well_represented_data, final_duplicated_data], ignore_index=True)\n","\n","# Create the new file CSV\n","final_file_path = '/kaggle/working/modified_train_labels.csv'\n","final_data.to_csv(final_file_path, index=False)\n","\n","# New Training Dataset\n","data_modified = pd.read_csv(final_file_path)\n","num_rows_modified = len(data_modified)\n","print(f\"Number of images in modified file: {num_rows_modified}\")\n","images_per_label_mod = data_modified['label'].value_counts()\n","print(\"Number of images per label (ascending order):\")\n","print(images_per_label_mod.sort_values())"],"metadata":{"trusted":true,"id":"bj4PbAhlnSHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Here the class 'FoodDataset is created.\n","It provides an appropriate way to manage images.\n","'''\n","class FoodDataset(Dataset):\n","\n","    def __init__(self, root_dir, transform, split):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory containing the images.\n","            transform (callable, optional): Transformation to be applied to the images.\n","            split (string): Split type (\"train\" or \"val\").\n","        \"\"\"\n","        self.split = split\n","        self.root_dir = root_dir\n","        self.data = self.get_data_train()\n","        self.data_v = self.get_data_val()\n","\n","        if split == \"train\":\n","            self.data = self.data\n","        elif split == \"val\":\n","            self.data = self.data_v\n","        self.transform = transform\n","\n","    def get_data_train(self):\n","        df = pd.read_csv('/kaggle/working/training_set.csv')\n","        # Construct full image paths using vectorized operations\n","        df['img_name'] = self.root_dir + \"/\" + df['img_name']\n","        # Filter for images with existing paths using vectorized boolean indexing\n","        df = df[df['img_name'].apply(os.path.exists)]\n","        return df.to_numpy()  # Return preprocessed data as a NumPy array\n","\n","    def get_data_val(self):\n","        df = pd.read_csv('/kaggle/working/validation_set.csv')\n","        # Construct full image paths using vectorized operations\n","        df['img_name'] = self.root_dir + \"/\" + df['img_name']\n","        # Filter for images with existing paths using vectorized boolean indexing\n","        df = df[df['img_name'].apply(os.path.exists)]\n","        return df.to_numpy()  # Return preprocessed data as a NumPy array\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset (number of samples).\n","\n","        This method overrides the default behavior of `len` for the dataset object.\n","        It simply returns the length of the internal `data` list, which represents\n","        the preprocessed data after loading and filtering.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a sample at a given index.\n","\n","        This method overrides the default behavior of indexing for the dataset object.\n","        It takes an index `idx` and performs the following:\n","            1. Accesses the image name and num_class at the specified index from `self.data`.\n","            2. Opens the image using `Image.open` with the full path constructed by\n","               combining `self.root_dir` and `img_name`.\n","            3. Applies the defined transformation (`self.transform`) to the image.\n","            4. Creates a dictionary `sample` containing the preprocessed image (`image`)\n","               and the num_class as a PyTorch tensor (`torch.tensor(num_class).float()`).\n","            5. Returns the constructed `sample` dictionary.\n","        \"\"\"\n","        img_name, num_class = self.data[idx]\n","        image = Image.open(os.path.join(self.root_dir, img_name))\n","        image = self.transform(image)\n","\n","        sample = {'img_name': image, 'labels': torch.tensor(num_class).float()}\n","        return sample\n"],"metadata":{"trusted":true,"id":"pwVwXoX-nSHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Functions for data transformation are provided,\n","distinguishing between trainin and validation sets.\n","Creation of Dataloaders.\n","'''\n","transform_train = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize images to 256x256\n","    #transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with a probability of 50%\n","    #transforms.RandomAffine(degrees=0, shear=0.2),  # Apply random shear transformations\n","    #transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0), ratio=(1.0, 1.0)),  # Apply random zoom transformations\n","    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n","    transforms.Normalize(  # Normalize pixel values based on ImageNet statistics\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","transform_val = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resize images to 256x256 (consistent with training)\n","    transforms.ToTensor(),  # Convert PIL images to PyTorch tensors\n","    transforms.Normalize(  # Normalize pixel values using the same statistics\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","\n","# Set batch size\n","bs = 30\n","\n","# Create datasets for training and validation\n","trainset = FoodDataset(\"/kaggle/working/train_set\", transform_train, split=\"train\")\n","valset = FoodDataset(\"/kaggle/working/train_set\", transform_val, split=\"val\")\n","\n","# Create data loaders for efficient batch training and evaluation\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n","\n","# Print dataset and dataloader lengths (number of samples and batches)\n","print(f\"Number of training samples: {len(trainloader) * bs}\")\n","print(f\"Number of validation samples: {len(valloader)}\")"],"metadata":{"trusted":true,"id":"iSzVCsX3nSHP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CNN"],"metadata":{"id":"bE3Yg6HZoD4v"}},{"cell_type":"code","source":["'''\n","Here the architecture of the CNN is provided.\n","Functions for SSL pretext task are present.\n","'''\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Let's create a modified version of MobileNetV2\n","        self.features = nn.Sequential(\n","            Conv2dNormActivation(3, 32, kernel_size=3, stride=2, padding=1),\n","            InvertedResidual(32, 16, stride=1, expand_ratio=1),\n","            InvertedResidual(16, 24, stride=2, expand_ratio=6),\n","            InvertedResidual(24, 24, stride=1, expand_ratio=6),\n","            InvertedResidual(24, 32, stride=2, expand_ratio=6),\n","            InvertedResidual(32, 32, stride=1, expand_ratio=6),\n","            InvertedResidual(32, 32, stride=1, expand_ratio=6),\n","            InvertedResidual(32, 64, stride=2, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 64, stride=1, expand_ratio=6),\n","            InvertedResidual(64, 96, stride=1, expand_ratio=6),\n","            InvertedResidual(96, 96, stride=2, expand_ratio=6),\n","            InvertedResidual(96, 96, stride=2, expand_ratio=6),\n","            Conv2dNormActivation(96, 800, kernel_size=1, stride=1, padding=0)\n","        )\n","\n","        # Definition of classifier\n","        self.classifier_puzzle = self._create_classifier(4)\n","\n","    def _create_classifier(self, num_classes):\n","        return nn.Sequential(\n","            nn.Linear(800, 445),\n","            nn.GELU(),\n","            nn.Linear(445, 32),\n","            nn.GELU(),\n","            nn.Linear(32, num_classes)\n","        )\n","    def forward_shared(self, x):\n","        x = self.features(x)\n","        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).flatten(1)\n","        return x\n","\n","    def forward_puzzle(self, x):\n","        x = self.forward_shared(x)\n","        x = self.classifier_puzzle(x)\n","        return x\n","\n","\n","    def _preprocess(self, images):\n","        \"\"\"\n","        Preprocess the input images to generate rotated images and create corresponding targets.\n","\n","        Parameters:\n","            images (Tensor): Batch of input images.\n","\n","        Returns:\n","            images_batch (Tensor): Batch of rotated images.\n","            targets (Tensor): Corresponding targets for the original image rotations.\n","        \"\"\"\n","        batch_size = images.shape[0]\n","\n","        # Generate rotated images\n","        images_90 = torch.flip(images.transpose(2, 3), (2,))\n","        images_180 = torch.flip(images, (2, 3))\n","        images_270 = torch.flip(images, (2,)).transpose(2, 3)\n","\n","        # Concatenate original and rotated images\n","        images_batch = torch.cat((images, images_90, images_180, images_270), dim=0)\n","\n","        # Create targets for rotations\n","        targets = torch.arange(4).long().repeat(batch_size)\n","        targets = targets.view(batch_size, 4).transpose(0, 1)\n","        targets = targets.contiguous().view(-1)\n","\n","        return images_batch, targets\n","\n","net = Net()\n","net.to(device)\n"],"metadata":{"trusted":true,"id":"KPuwiutGnSHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRAINING and EVALUATION"],"metadata":{"id":"61AjEHXUoTJC"}},{"cell_type":"code","source":["'''\n","Define training parameters (epochs, loss function, optimizer, and scheduler)\n","'''\n","epochs = 12  # Number of training epochs\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer_shared = optim.Adam(net.features.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n","scheduler_shared = CosineAnnealingLR(optimizer_shared,\n","                              T_max=len(trainloader) * epochs,  # Maximum number of iterations for scheduler\n","                              eta_min=1e-5)  # Minimum learning rate for scheduler\n","optimizer_puzzle = optim.Adam(net.classifier_puzzle.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n","scheduler_puzzle = CosineAnnealingLR(optimizer_puzzle,\n","                              T_max=len(trainloader) * epochs,  # Maximum number of iterations for scheduler\n","                              eta_min=1e-5)  # Minimum learning rate for scheduler\n"],"metadata":{"trusted":true,"id":"Vqpo83cTnSHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Training and Validation section.\n","'''\n","\n","all_preds = []\n","all_labels = []\n","\n","# Training loop\n","for epoch in range(epochs):  # Loop over the dataset for multiple epochs\n","    running_loss = []  # List to store training loss for each batch\n","    net.train()  # Set the model to training mode\n","    for i, data in enumerate(trainloader):\n","        # Get inputs and labels from the data loader\n","        inputs, labels = data[\"img_name\"], data[\"labels\"]\n","        inputs = inputs.to(device)\n","        labels = labels.to(device).long()\n","        # Zero the parameter gradients before each backward pass\n","        optimizer_shared.zero_grad()\n","        optimizer_puzzle.zero_grad()\n","        # Forward pass\n","        shuffled_pieces, targets = net._preprocess(inputs)\n","        shuffled_pieces, targets = shuffled_pieces.to(device), targets.to(device)\n","\n","        outputs = net.forward_puzzle(shuffled_pieces)\n","        outputs = outputs.to(device)\n","        # Calculate Loss\n","        loss = criterion(outputs.squeeze(), targets)\n","        # Backward pass and parameter update\n","        loss.backward()\n","        optimizer_shared.step()\n","        optimizer_puzzle.step()\n","        scheduler_shared.step()\n","        scheduler_puzzle.step()\n","        # Print statistics (every 10% of the training data)\n","        running_loss.append(loss.item())\n","\n","        if (i + 1) % (len(trainloader) // 10) == 0:  # Every 10% of the epoch\n","            print('%d, [%d, %d] loss puzzle: %.4f\\tlr: %.6f' %\n","                  (epoch + 1, i + 1, len(trainloader), np.mean(running_loss), optimizer_puzzle.param_groups[-1]['lr']))\n","            running_loss = []\n","    # Validation loop\n","    running_loss = []  # List to store validation loss for each batch\n","\n","    net.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    for i, data in enumerate(valloader):\n","        # Get inputs and labels from the data loader\n","        inputs, labels = data[\"img_name\"], data[\"labels\"]\n","        inputs = inputs.to(device)\n","        labels = labels.to(device).long()\n","\n","        shuffled_pieces, targets = net._preprocess(inputs)\n","        shuffled_pieces, targets = shuffled_pieces.to(device), targets.to(device)\n","        # Forward pass with gradient suppression\n","        with torch.no_grad():\n","            outputs = net.forward_puzzle(shuffled_pieces)  # Get model predictions without calculating gradients\n","            outputs = outputs.to(device)\n","\n","            # Finds the class with the highest probability for each image in the batch.\n","            _, predicted = torch.max(outputs.cpu(), 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(targets.cpu().numpy())\n","            # Update total number of test images\n","            total += targets.size(0)  # label.size(0) gives the batch size\n","            # Count correct predictions\n","            correct += (predicted == targets.cpu()).sum().item()  # Count true positives\n","\n","\n","        loss = criterion(outputs, targets)  # Calculate loss\n","        running_loss.append(loss.item())\n","\n","    val_loss = np.mean(running_loss)\n","\n","    print('Validation loss puzzle: %.6f' % (val_loss))\n","    # Calculate and print accuracy\n","    accuracy = 100 * correct / total\n","    print(f'Validation accuracy puzzle: {accuracy:.2f} %')\n","\n","\n","\n","print('Finished Training')\n","torch.save(net.state_dict(), \"net_last.pth\")  # Save the last model state"],"metadata":{"trusted":true,"id":"TWUKytVRnSHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FEATURES EXTRACTION"],"metadata":{"id":"emnndv9VownF"}},{"cell_type":"code","source":["'''\n","Extract features from the model\n","'''\n","\n","def extract_features(model, dataloader, device):\n","    model.eval() # Evaluation mode\n","    # List to save features and labels\n","    features = []\n","    labels = []\n","    # Gradient suppression\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs, lbls = data[\"img_name\"], data[\"labels\"]\n","            inputs = inputs.to(device)\n","            lbls = lbls.to(device).long()\n","            # Forward pass\n","            feats = model.forward_shared(inputs)\n","            features.append(feats.cpu().numpy())\n","            labels.append(lbls.cpu().numpy())\n","    # Creation of the final lists\n","    features = np.concatenate(features, axis=0)\n","    labels = np.concatenate(labels, axis=0)\n","\n","    return features, labels"],"metadata":{"id":"ln2xOUnunSHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Upload the model and the features\n","'''\n","\n","model = Net()\n","model.load_state_dict(torch.load('net_last.pth'))\n","model.to(device)\n","\n","features, labels = extract_features(model, valloader, device)\n","\n","np.save('puzzle_features.npy', features)\n","np.save('puzzle_labels.npy', labels)\n","\n","print('Done')"],"metadata":{"trusted":true,"id":"QjTGN1z5nSHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SVM"],"metadata":{"id":"krF-7QYipAz3"}},{"cell_type":"code","source":["'''\n","Training of the SVM with extracted features.\n","Evaluation.\n","'''\n","features = np.load('puzzle_features.npy')\n","labels = np.load('puzzle_labels.npy')\n","\n","#(80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","# Initialize SVM\n","classifier = svm.SVC(kernel='linear')\n","\n","# Training\n","classifier.fit(X_train, y_train)\n","\n","# Prediction\n","y_pred = classifier.predict(X_test)\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Test accuracy: {accuracy * 100:.2f}%')"],"metadata":{"trusted":true,"id":"zQQ7uHTOnSHR"},"execution_count":null,"outputs":[]}]}